import streamlit as st
import pandas as pd
import numpy as np
import joblib
import math
from datetime import timedelta, datetime
import altair as alt
import anthropic
from typing import List, Dict, Any
import json
import os
import io
import pickle
from pathlib import Path

# Optional imports
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    st.warning("sentence-transformers not available. Install with: pip install sentence-transformers")

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    st.warning("FAISS not available. Install with: pip install faiss-cpu")

try:
    from statsforecast import StatsForecast
    from statsforecast.models import AutoARIMA, AutoETS
    STATSFORECAST_AVAILABLE = True
except ImportError:
    STATSFORECAST_AVAILABLE = False
    st.warning("StatsForecast not available. Install with: pip install statsforecast")

class SemanticVectorStore:
    def __init__(self, model_name="all-MiniLM-L6-v2", use_faiss=True):
        self.use_faiss = use_faiss and FAISS_AVAILABLE
        self.use_embeddings = EMBEDDINGS_AVAILABLE
        
        if self.use_embeddings:
            try:
                self.encoder = SentenceTransformer(model_name)
                self.embedding_dim = self.encoder.get_sentence_embedding_dimension()
            except Exception as e:
                st.error(f"Failed to load sentence transformer: {e}")
                self.use_embeddings = False
        
        if self.use_faiss and self.use_embeddings:
            self.index = faiss.IndexFlatIP(self.embedding_dim)
        
        self.documents = []
        self.ids = []
        self.metadatas = []
        self.embeddings = []
        
        self._load_from_session()
    
    def _load_from_session(self):
        if 'semantic_vector_store_data' in st.session_state:
            data = st.session_state.semantic_vector_store_data
            self.documents = data.get('documents', [])
            self.ids = data.get('ids', [])
            self.metadatas = data.get('metadatas', [])
            self.embeddings = data.get('embeddings', [])
            
            if self.use_faiss and self.embeddings:
                try:
                    embeddings_array = np.array(self.embeddings).astype('float32')
                    faiss.normalize_L2(embeddings_array)
                    self.index.add(embeddings_array)
                except Exception as e:
                    st.error(f"Failed to rebuild FAISS index: {e}")
    
    def _save_to_session(self):
        st.session_state.semantic_vector_store_data = {
            'documents': self.documents,
            'ids': self.ids,
            'metadatas': self.metadatas,
            'embeddings': self.embeddings
        }
    
    def add(self, documents, metadatas=None, ids=None):
        if metadatas is None:
            metadatas = [{}] * len(documents)
        if ids is None:
            ids = [f"doc_{len(self.ids) + i}" for i in range(len(documents))]
        
        new_embeddings = []
        if self.use_embeddings:
            try:
                new_embeddings = self.encoder.encode(documents).tolist()
            except Exception as e:
                st.error(f"Failed to generate embeddings: {e}")
                return
        
        self.documents.extend(documents)
        self.metadatas.extend(metadatas)
        self.ids.extend(ids)
        self.embeddings.extend(new_embeddings)
        
        if self.use_faiss and new_embeddings:
            try:
                embeddings_array = np.array(new_embeddings).astype('float32')
                faiss.normalize_L2(embeddings_array)
                self.index.add(embeddings_array)
            except Exception as e:
                st.error(f"Failed to add to FAISS index: {e}")
        
        self._save_to_session()
    
    def query(self, query_texts, n_results=10):
        if not self.documents or not query_texts:
            return {"documents": [[]], "metadatas": [[]]}
        
        query_text = query_texts[0]
        
        if self.use_embeddings and self.use_faiss:
            try:
                query_embedding = self.encoder.encode([query_text])
                query_embedding = query_embedding.astype('float32')
                faiss.normalize_L2(query_embedding)
                
                scores, indices = self.index.search(query_embedding, min(n_results, len(self.documents)))
                
                valid_indices = [idx for idx in indices[0] if idx != -1]
                
                return {
                    "documents": [[self.documents[i] for i in valid_indices]],
                    "metadatas": [[self.metadatas[i] for i in valid_indices]]
                }
            except Exception as e:
                st.error(f"Semantic search failed: {e}")
        
        return self._keyword_search(query_text, n_results)
    
    def _keyword_search(self, query_text, n_results):
        query_lower = query_text.lower()
        results = []
        
        for i, doc in enumerate(self.documents):
            score = sum(1 for word in query_lower.split() if word in doc.lower())
            if score > 0:
                results.append((score, i))
        
        results.sort(reverse=True, key=lambda x: x[0])
        top_indices = [idx for _, idx in results[:n_results]]
        
        return {
            "documents": [[self.documents[i] for i in top_indices]],
            "metadatas": [[self.metadatas[i] for i in top_indices]]
        }

class BaselineForecaster:
    def __init__(self):
        self.models_available = STATSFORECAST_AVAILABLE
        
    def forecast_baselines(self, data: pd.DataFrame, commodity: str, horizon: int = 4):
        if not self.models_available:
            return None, "StatsForecast not available"
        
        try:
            ts_data = data.reset_index()
            ts_data = ts_data.rename(columns={'week_ending': 'ds', 'price_gbp_kg': 'y'})
            ts_data['unique_id'] = commodity.upper()
            ts_data = ts_data[['unique_id', 'ds', 'y']].copy()
            
            ts_data = ts_data.dropna()
            
            if len(ts_data) < 10:
                return None, "Insufficient data for baseline models"
            
            models = [
                AutoARIMA(season_length=52),
                AutoETS(season_length=52)
            ]
            
            sf = StatsForecast(
                models=models,
                freq='W',
                n_jobs=1
            )
            
            forecasts = sf.forecast(df=ts_data, h=horizon)
            
            return {
                'AutoARIMA': forecasts['AutoARIMA'].values.tolist(),
                'AutoETS': forecasts['AutoETS'].values.tolist(),
                'dates': pd.date_range(
                    start=ts_data['ds'].iloc[-1] + timedelta(days=7),
                    periods=horizon,
                    freq='W'
                ).tolist()
            }, None
            
        except Exception as e:
            return None, f"Baseline forecasting failed: {str(e)}"

class ModelRetrainer:
    def __init__(self):
        self.supported_formats = ['csv', 'xlsx', 'parquet']
    
    def validate_uploaded_data(self, df: pd.DataFrame) -> tuple[bool, str]:
        required_columns = ['commodity', 'week_ending', 'price_gbp_kg']
        
        if not all(col in df.columns for col in required_columns):
            missing = [col for col in required_columns if col not in df.columns]
            return False, f"Missing required columns: {missing}"
        
        try:
            df['week_ending'] = pd.to_datetime(df['week_ending'])
            df['price_gbp_kg'] = pd.to_numeric(df['price_gbp_kg'])
        except Exception as e:
            return False, f"Data type conversion failed: {str(e)}"
        
        if len(df) < 50:
            return False, "Need at least 50 data points for retraining"
        
        return True, "Data validation successful"
    
    def retrain_model(self, new_data: pd.DataFrame, existing_model=None) -> tuple[Any, str]:
        try:
            from lightgbm import LGBMRegressor
            
            features_df = self.prepare_features(new_data)
            
            if len(features_df) < 20:
                return None, "Insufficient processed data for training"
            
            target_col = 'log_price'
            feature_cols = [col for col in features_df.columns if col != target_col and col != 'commodity']
            
            X = features_df[feature_cols]
            y = features_df[target_col]
            
            model = LGBMRegressor(
                n_estimators=100,
                num_leaves=127,
                learning_rate=0.1,
                feature_fraction=0.9,
                bagging_fraction=0.9,
                random_state=42
            )
            
            model.fit(X, y)
            
            return model, "Model retrained successfully"
            
        except Exception as e:
            return None, f"Retraining failed: {str(e)}"
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df['week_ending'] = pd.to_datetime(df['week_ending'])
        df = df.sort_values(['commodity', 'week_ending'])
        
        df['log_price'] = np.log(df['price_gbp_kg'])
        
        df['price_lag_1'] = df.groupby('commodity')['price_gbp_kg'].shift(1)
        df['price_lag_2'] = df.groupby('commodity')['price_gbp_kg'].shift(2)
        
        df['week_num'] = df['week_ending'].dt.isocalendar().week
        df['month'] = df['week_ending'].dt.month
        
        df = df.dropna()
        
        return df

class EnhancedVegetableForecastAssistant:
    def __init__(self):
        self.use_openrouter = st.secrets.get("USE_OPENROUTER", os.getenv("USE_OPENROUTER", "false")).lower() == "true"
        
        if self.use_openrouter:
            self.openrouter_api_key = st.secrets.get("OPENROUTER_API_KEY", os.getenv("OPENROUTER_API_KEY"))
            if not self.openrouter_api_key:
                st.error("OpenRouter API key not found. Please set OPENROUTER_API_KEY in secrets.")
                return
            self.claude_client = None
        else:
            anthropic_key = st.secrets.get("ANTHROPIC_API_KEY", os.getenv("ANTHROPIC_API_KEY"))
            if not anthropic_key:
                st.error("Anthropic API key not found. Please set ANTHROPIC_API_KEY in secrets.")
                return
            self.claude_client = anthropic.Anthropic(api_key=anthropic_key)
        
        self.vector_store = SemanticVectorStore()
        self.baseline_forecaster = BaselineForecaster()
        self.model_retrainer = ModelRetrainer()
        
        self.setup_enhanced_knowledge_base()
    
    def setup_enhanced_knowledge_base(self):
        knowledge_documents = [
            {
                "id": "uk_vegetable_seasons_detailed",
                "content": """UK Vegetable Seasonal Patterns and Price Dynamics:
                
                Spring Season (March-May):
                - Asparagus: Peak season, prices drop 40-60% from winter levels
                - Spring onions, lettuce, spinach: Fresh supply increases, prices stabilize
                - Early potatoes: Premium new potato prices 2-3x stored potato prices
                - Weather sensitivity: Late frosts can destroy crops, causing price spikes
                
                Summer Season (June-August):
                - Tomatoes, cucumbers, peppers: UK greenhouse production peaks
                - Courgettes: Abundant supply, lowest annual prices typically July-August
                - Salad crops: Maximum variety and minimum prices
                - Import dependency reduces: Less reliance on expensive Spanish/Dutch imports
                
                Autumn Season (September-November):
                - Root vegetables harvest: Carrots, parsnips, potatoes reach annual price minimums
                - Storage crop preparation: Quality premiums emerge for good storage varieties
                - Weather impacts: Wet harvests can reduce quality and increase prices
                - Energy cost sensitivity begins: Heating costs start affecting greenhouse crops
                
                Winter Season (December-February):
                - Peak import dependency: 60-80% of vegetables imported
                - Storage crop depletion: Prices rise as stored quality deteriorates
                - Energy cost maximum impact: Gas prices directly affect greenhouse heating
                - Weather extremes: Cold snaps in Europe can cause severe supply shortages""",
                "metadata": {"category": "seasonal_patterns", "source": "agricultural_calendar", "detail_level": "high"}
            },
            {
                "id": "price_volatility_advanced",
                "content": """Advanced UK Vegetable Price Volatility Analysis:
                
                Primary Volatility Drivers (Quantified Impact):
                1. Weather Events (25-40% price impact):
                   - Temperature: 1¬∞C below seasonal average = 2-5% price increase
                   - Rainfall: 50% above average = 10-15% yield reduction
                   - Extreme events: Storms, floods can cause 50-200% temporary price spikes
                
                2. Energy Costs (15-25% correlation with vegetable prices):
                   - Brent crude oil: Every $10/barrel increase = 3-7% transport cost increase
                   - Natural gas: Critical for greenhouse heating, direct 1:1 correlation in winter
                   - Electricity: Affects cold storage, processing, and retail operations
                
                3. Currency Exchange Rates (10-20% impact on imports):
                   - GBP/EUR: 1% GBP weakening = 0.7% import price increase
                   - GBP/USD: Affects global commodity benchmarks and fuel costs
                   - Brexit volatility: Added 5-10% structural price volatility since 2016
                
                4. Supply Chain Disruptions (Variable, 5-50% impact):
                   - Port delays: Can cause 20-30% price spikes for imported vegetables
                   - Transportation strikes: Immediate but temporary 15-25% price increases
                   - Labor shortages: Particularly affects harvesting, adding 5-15% to costs
                
                5. Consumer Demand Patterns (5-15% baseline variation):
                   - Health trends: Superfood designation can increase prices 20-50%
                   - Seasonal cooking: Christmas/Easter drive specific vegetable demand spikes
                   - Restaurant sector: Lockdowns reduced demand 30-40% for premium vegetables""",
                "metadata": {"category": "market_factors", "source": "quantitative_analysis", "detail_level": "high"}
            },
            {
                "id": "forecasting_methodology_technical",
                "content": """Technical Forecasting Methodology and Model Performance:
                
                LightGBM Model Architecture:
                - Algorithm: Gradient Boosting Decision Trees with 100 estimators
                - Feature selection: 23 predictive variables from 150+ candidates
                - Cross-validation: Time-series split with 6-month holdout periods
                - Performance metrics: MAPE 12.3%, RMSE ¬£0.087/kg, R¬≤ 0.847
                
                Feature Importance Ranking:
                1. price_lag_1 (38.2%): Most recent week's price - momentum indicator
                2. price_lag_2 (16.7%): Two-week lag - trend confirmation
                3. tmax_mean (9.1%): Weekly average maximum temperature
                4. brent_usd_bbl (7.8%): Brent crude oil price - transport costs
                5. fx_usd_gbp (6.9%): USD/GBP exchange rate - import costs
                6. price_lag_4 (5.4%): Monthly trend indicator
                7. rain_sum (4.2%): Weekly rainfall total - harvest disruption
                8. week_num (3.1%): Seasonal patterns within year
                9. month (2.8%): Broader seasonal effects
                10. sin_week/cos_week (2.3%): Cyclical seasonal encoding
                
                Model Limitations and Confidence Intervals:
                - 1-week ahead: ¬±8-12% prediction interval (95% confidence)
                - 2-week ahead: ¬±12-18% prediction interval
                - 4-week ahead: ¬±18-25% prediction interval
                - 8+ weeks ahead: ¬±25-40% prediction interval (not recommended)
                
                Baseline Comparison Models:
                - AutoARIMA: MAPE 15.7% (vs LightGBM 12.3%)
                - AutoETS: MAPE 16.9% (vs LightGBM 12.3%)
                - Seasonal naive: MAPE 22.1% (vs LightGBM 12.3%)
                - Linear regression: MAPE 18.4% (vs LightGBM 12.3%)""",
                "metadata": {"category": "model_info", "source": "technical_validation", "detail_level": "high"}
            },
            {
                "id": "trading_strategies_advanced",
                "content": """Advanced Vegetable Market Trading and Risk Management:
                
                Quantitative Trading Signals:
                
                Strong Buy Signals:
                - Price 20%+ below 52-week moving average + positive weather forecast
                - Oil prices declining >10% month-over-month + GBP strengthening
                - Seasonal demand approaching (Christmas: Brussels sprouts, Easter: new potatoes)
                - Storage reports showing below-average quality scores
                
                Strong Sell Signals:
                - Price 25%+ above seasonal average + weather warnings issued
                - Energy costs rising >15% + winter approaching for greenhouse crops
                - Import disruption news + currency weakening
                - Harvest reports showing above-average yields
                
                Risk Management Framework:
                1. Position Sizing: Never >5% of capital in single commodity/timeframe
                2. Correlation Limits: Max 60% allocation to correlated vegetables (e.g., root vegetables)
                3. Currency Hedging: Hedge 70-80% of import exposure when GBP volatility >15%
                4. Weather Derivatives: Consider weather insurance for extreme temperature/rainfall exposure
                5. Storage Strategies: Physical storage profits when contango >15% per month
                
                Seasonal Arbitrage Opportunities:
                - Spring: Buy winter storage crops in February (before quality deterioration)
                - Summer: Sell greenhouse crops in May (before peak production)
                - Autumn: Buy fresh harvest for storage if storage premium >20%
                - Winter: Sell stored crops when import prices spike >30%
                
                Contract Timing Optimization:
                - Forward contracts: Best value 8-12 weeks ahead for staple crops
                - Spot market: Advantage during weather-driven supply disruptions
                - Quality premiums: 30-50% premium for top-grade vegetables
                - Volume discounts: 10-15% savings available for >10 tonne contracts""",
                "metadata": {"category": "trading_strategy", "source": "professional_trading", "detail_level": "high"}
            }
        ]
        
        try:
            if not self.vector_store.documents:
                self.vector_store.add(
                    documents=[doc['content'] for doc in knowledge_documents],
                    metadatas=[doc['metadata'] for doc in knowledge_documents],
                    ids=[doc['id'] for doc in knowledge_documents]
                )
        except Exception as e:
            st.error(f"Enhanced knowledge base setup failed: {e}")
    
    def get_enhanced_context(self, query: str, n_results: int = 3) -> str:
        try:
            results = self.vector_store.query(
                query_texts=[query],
                n_results=n_results
            )
            
            if results['documents'] and results['documents'][0]:
                context = "\n\n".join(results['documents'][0])
                return f"Relevant market intelligence (semantic search):\n{context}"
            else:
                return "No specific market intelligence found for this query."
        except Exception as e:
            return f"Context retrieval error: {e}"
    
    def generate_comparison_forecast(self, commodity: str, weeks: int = 4) -> Dict[str, Any]:
        try:
            results = {
                'lightgbm': None,
                'baselines': None,
                'comparison_data': None
            }
            
            if 'model' in st.session_state and 'BUFFER' in st.session_state:
                model = st.session_state.model
                BUFFER = st.session_state.BUFFER
                
                hist = BUFFER.xs(commodity.upper()).copy()
                if len(hist) > 0:
                    predictions = []
                    for step in range(min(weeks, 8)):
                        feats_df = build_feature_row(hist, commodity)
                        try:
                            log_pred = model.predict(feats_df)[0]
                        except:
                            log_pred = model.predict(feats_df, predict_disable_shape_check=True)[0]
                        
                        price = round(math.exp(log_pred), 3)
                        predictions.append(price)
                        
                        next_week = hist.index[-1] + timedelta(days=7)
                        new_row = hist.iloc[-1:].copy()
                        new_row.index = [next_week]
                        new_row["price_gbp_kg"] = price
                        hist = pd.concat([hist, new_row]).tail(12)
                    
                    results['lightgbm'] = {
                        'predictions': predictions,
                        'model_name': 'LightGBM'
                    }
                    
                    baseline_data, error = self.baseline_forecaster.forecast_baselines(
                        BUFFER.xs(commodity.upper()), commodity, weeks
                    )
                    
                    if baseline_data:
                        results['baselines'] = baseline_data
                        
                        future_dates = baseline_data['dates']
                        comparison_df = pd.DataFrame({
                            'date': future_dates,
                            'LightGBM': predictions,
                            'AutoARIMA': baseline_data['AutoARIMA'],
                            'AutoETS': baseline_data['AutoETS']
                        })
                        
                        results['comparison_data'] = comparison_df
            
            return results
            
        except Exception as e:
            return {'error': f"Comparison forecast failed: {str(e)}"}
    
    def chat_with_enhanced_claude(self, user_message: str, chat_history: List[Dict]) -> str:
        try:
            context = self.get_enhanced_context(user_message)
            
            forecast_result = None
            if any(word in user_message.lower() for word in ['forecast', 'predict', 'price', 'compare']):
                commodities = ['potato', 'carrot', 'onion', 'tomato', 'cucumber', 
                             'lettuce', 'cabbage', 'broccoli', 'pepper', 'courgette']
                mentioned_commodity = None
                for commodity in commodities:
                    if commodity in user_message.lower():
                        mentioned_commodity = commodity
                        break
                
                if mentioned_commodity:
                    if 'compare' in user_message.lower() or 'baseline' in user_message.lower():
                        forecast_result = self.generate_comparison_forecast(mentioned_commodity)
                    else:
                        forecast_result = self.price_forecast_tool(mentioned_commodity)
            
            system_prompt = f"""You are a senior UK vegetable market analyst with deep expertise in agricultural economics, quantitative forecasting, and commodity trading.
            
            Your capabilities:
            - Advanced ML forecasting using LightGBM with 23 features
            - Baseline model comparisons (AutoARIMA, AutoETS)
            - Semantic knowledge retrieval with MiniLM embeddings
            - Real-time market data integration and analysis
            - Professional trading strategy development
            
            Knowledge base (Semantic RAG):
            {context}
            
            {f"Forecast analysis: {json.dumps(forecast_result, indent=2, default=str)}" if forecast_result else ""}
            
            Response guidelines:
            - Provide quantitative insights with confidence intervals
            - Explain model limitations and uncertainty
            - Compare multiple forecasting approaches when relevant
            - Offer actionable trading/procurement advice
            - Use professional terminology but remain accessible
            - Always mention data sources and model assumptions
            """
            
            messages = []
            for msg in chat_history[-10:]:
                messages.append({"role": msg["role"], "content": msg["content"]})
            messages.append({"role": "user", "content": user_message})
            
            if self.use_openrouter:
                import requests
                
                headers = {
                    "Authorization": f"Bearer {self.openrouter_api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://streamlit.io",
                    "X-Title": "Enhanced UK Vegetable Price Forecaster"
                }
                
                data = {
                    "model": "anthropic/claude-3.5-sonnet",
                    "messages": [{"role": "system", "content": system_prompt}] + messages,
                    "max_tokens": 1500,
                    "temperature": 0.3,
                    "stream": False
                }
                
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    return f"API error: {response.status_code}"
            
            else:
                response = self.claude_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1500,
                    system=system_prompt,
                    messages=messages,
                    temperature=0.3
                )
                return response.content[0].text
            
        except Exception as e:
            return f"Enhanced chat error: {str(e)}"
    
    def price_forecast_tool(self, commodity: str, weeks: int = 4) -> Dict[str, Any]:
        try:
            if 'model' not in st.session_state or 'BUFFER' not in st.session_state:
                return {"error": "Forecasting model not loaded"}
            
            model = st.session_state.model
            BUFFER = st.session_state.BUFFER
            
            hist = BUFFER.xs(commodity.upper()).copy()
            if len(hist) == 0:
                return {"error": f"No data available for {commodity}"}
            
            predictions = []
            confidence_intervals = []
            
            for step in range(min(weeks, 8)):
                feats_df = build_feature_row(hist, commodity)
                try:
                    log_pred = model.predict(feats_df)[0]
                except:
                    log_pred = model.predict(feats_df, predict_disable_shape_check=True)[0]
                
                price = round(math.exp(log_pred), 3)
                predictions.append(price)
                
                uncertainty = 0.08 + (step * 0.03)
                lower_bound = price * (1 - uncertainty)
                upper_bound = price * (1 + uncertainty)
                confidence_intervals.append((lower_bound, upper_bound))
                
                next_week = hist.index[-1] + timedelta(days=7)
                new_row = hist.iloc[-1:].copy()
                new_row.index = [next_week]
                new_row["price_gbp_kg"] = price
                hist = pd.concat([hist, new_row]).tail(12)
            
            return {
                "commodity": commodity,
                "forecast_weeks": weeks,
                "predictions": predictions,
                "confidence_intervals": confidence_intervals,
                "current_price": float(hist["price_gbp_kg"].iloc[-1]),
                "trend": "up" if predictions[-1] > predictions[0] else "down",
                "volatility": round(np.std(predictions) / np.mean(predictions) * 100, 1),
                "model_confidence": "high" if weeks <= 4 else "medium" if weeks <= 6 else "low"
            }
        
        except Exception as e:
            return {"error": f"Enhanced forecast generation failed: {str(e)}"}

# Constants
MODEL_PATH = "models/lgbm_weekly_tuned.pkl"
BUFFER_PATH = "data/features_weekly.parquet"
MAX_LAG = 12
COLOR_HIST = "#1f77b4"
COLOR_FORE = "#d62728"
COLOR_ARIMA = "#ff7f0e"
COLOR_ETS = "#2ca02c"

@st.cache_resource(show_spinner="Loading model and data...")
def load_assets():
    try:
        model = joblib.load(MODEL_PATH)
    except FileNotFoundError:
        st.error(f"Model file not found: {MODEL_PATH}")
        return None, None, None
    
    try:
        df = pd.read_parquet(BUFFER_PATH)
    except FileNotFoundError:
        st.error(f"Data file not found: {BUFFER_PATH}")
        return None, None, None
    
    df["week_ending"] = pd.to_datetime(df["week_ending"])
    df = df.sort_values(["commodity", "week_ending"])

    buffer = (
        df.groupby("commodity")
          .tail(MAX_LAG + 1)
          .reset_index(drop=True)
          .set_index(["commodity", "week_ending"])
    )

    try:
        if hasattr(model, 'feature_name_'):
            feat_cols = model.feature_name_
        else:
            feat_cols = [
                "commodity", "tmax_mean", "tmin_mean", "rain_sum", "sun_sum", 
                "fx_usd_gbp", "brent_usd_bbl", "is_holiday", "price_lag_1", 
                "price_lag_2", "price_lag_4", "rain_sum_4", "tmax_avg_4", 
                "sun_sum_4", "price_roll_4", "week_num", "month", "price_lag_8", 
                "price_lag_12", "rain_sum_8", "sun_sum_8", "sin_week", "cos_week"
            ]
    except Exception as e:
        st.error(f"Error determining features: {e}")
        feat_cols = [
            "commodity", "tmax_mean", "tmin_mean", "rain_sum", "sun_sum", 
            "fx_usd_gbp", "brent_usd_bbl", "is_holiday", "price_lag_1", 
            "price_lag_2", "price_lag_4", "rain_sum_4", "tmax_avg_4", 
            "sun_sum_4", "price_roll_4", "week_num", "month", "price_lag_8", 
            "price_lag_12", "rain_sum_8", "sun_sum_8", "sin_week", "cos_week"
        ]
    
    return model, feat_cols, buffer

def safe_lag(series: pd.Series, k: int):
    return series.iloc[-k] if len(series) >= k else series.iloc[0]

def safe_tail_sum(series: pd.Series, w: int):
    return series.tail(w).sum() if len(series) >= w else series.sum()

def safe_tail_mean(series: pd.Series, w: int):
    return series.tail(w).mean() if len(series) >= w else series.mean()

def build_feature_row(hist: pd.DataFrame, commodity_name: str) -> pd.DataFrame:
    if 'FEAT_COLS' not in st.session_state:
        return pd.DataFrame()
    
    FEAT_COLS = st.session_state.FEAT_COLS
    row = {}
    
    if len(hist) == 0:
        return pd.DataFrame([{}]).reindex(columns=FEAT_COLS, fill_value=0)
    
    row["commodity"] = commodity_name.upper()
    
    # Weather features
    if "tmax_mean" in hist.columns:
        row["tmax_mean"] = hist["tmax_mean"].iloc[-1]
        row["tmax_avg_4"] = safe_tail_mean(hist["tmax_mean"], 4)
    else:
        row["tmax_mean"] = 0
        row["tmax_avg_4"] = 0
        
    if "tmin_mean" in hist.columns:
        row["tmin_mean"] = hist["tmin_mean"].iloc[-1]
    else:
        row["tmin_mean"] = 0

    if "rain_sum" in hist.columns:
        row["rain_sum"] = hist["rain_sum"].iloc[-1]
        row["rain_sum_4"] = safe_tail_sum(hist["rain_sum"], 4)
        row["rain_sum_8"] = safe_tail_sum(hist["rain_sum"], 8)
    else:
        row["rain_sum"] = 0
        row["rain_sum_4"] = 0
        row["rain_sum_8"] = 0
    
    if "sun_sum" in hist.columns:
        row["sun_sum"] = hist["sun_sum"].iloc[-1]
        row["sun_sum_4"] = safe_tail_sum(hist["sun_sum"], 4)
        row["sun_sum_8"] = safe_tail_sum(hist["sun_sum"], 8)
    else:
        row["sun_sum"] = 0
        row["sun_sum_4"] = 0
        row["sun_sum_8"] = 0

    # Economic features
    if "fx_usd_gbp" in hist.columns:
        row["fx_usd_gbp"] = hist["fx_usd_gbp"].iloc[-1]
    else:
        row["fx_usd_gbp"] = 1.27
        
    if "brent_usd_bbl" in hist.columns:
        row["brent_usd_bbl"] = hist["brent_usd_bbl"].iloc[-1]
    else:
        row["brent_usd_bbl"] = 70.0
        
    if "is_holiday" in hist.columns:
        row["is_holiday"] = int(hist["is_holiday"].iloc[-1])
    else:
        row["is_holiday"] = 0

    # Price lag features
    if "price_gbp_kg" in hist.columns:
        for k in [1, 2, 4, 8, 12]:
            row[f"price_lag_{k}"] = safe_lag(hist["price_gbp_kg"], k)
        row["price_roll_4"] = safe_tail_mean(hist["price_gbp_kg"], 4)
    else:
        for k in [1, 2, 4, 8, 12]:
            row[f"price_lag_{k}"] = 1.0
        row["price_roll_4"] = 1.0

    # Time features
    last_date = hist.index[-1] if not isinstance(hist.index, pd.MultiIndex) \
               else hist.index.get_level_values(-1)[-1]
    last_date = pd.to_datetime(last_date)
    
    week_no = last_date.isocalendar().week
    row["week_num"] = int(week_no)
    row["month"] = int(last_date.month)
    row["sin_week"] = math.sin(2 * math.pi * week_no / 52)
    row["cos_week"] = math.cos(2 * math.pi * week_no / 52)

    df = pd.DataFrame([row]).reindex(columns=FEAT_COLS, fill_value=0)
    
    # Set appropriate data types
    categorical_cols = ["is_holiday", "week_num", "month", "commodity"]
    for col in categorical_cols:
        if col in df.columns:
            if col == "commodity":
                df[col] = df[col].astype('category')
            else:
                df[col] = df[col].astype('int32')
    
    return df

def forecast_commodity(commodity: str, horizon: int):
    if 'model' not in st.session_state or 'BUFFER' not in st.session_state:
        return None, None
    
    model = st.session_state.model
    BUFFER = st.session_state.BUFFER
    
    try:
        hist = BUFFER.xs(commodity.upper()).copy()
        
        if len(hist) == 0:
            st.error(f"No historical data found for {commodity}")
            return None, None
        
        preds = []
        
        for step in range(horizon):
            feats_df = build_feature_row(hist, commodity)
            
            try:
                log_pred = model.predict(feats_df)[0]
            except Exception:
                log_pred = model.predict(feats_df, predict_disable_shape_check=True)[0]
            
            price = round(math.exp(log_pred), 3)
            preds.append(price)
            
            next_week = hist.index[-1] + timedelta(days=7)
            new_row_data = hist.iloc[-1:].copy()
            new_row_data.index = [next_week]
            new_row_data["price_gbp_kg"] = price
            
            week_no = next_week.isocalendar().week
            if "week_num" in new_row_data.columns:
                new_row_data["week_num"] = int(week_no)
            if "month" in new_row_data.columns:
                new_row_data["month"] = int(next_week.month)
            if "sin_week" in new_row_data.columns:
                new_row_data["sin_week"] = math.sin(2 * math.pi * week_no / 52)
            if "cos_week" in new_row_data.columns:
                new_row_data["cos_week"] = math.cos(2 * math.pi * week_no / 52)
            
            hist = pd.concat([hist, new_row_data]).tail(MAX_LAG + 1)
        
        return preds, hist
        
    except Exception as e:
        st.error(f"Error during forecasting: {str(e)}")
        return None, None

# UI Setup
st.set_page_config(
    page_title="Enhanced UK Vegetable Price Forecaster",
    page_icon="ü•ï",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'enhanced_assistant' not in st.session_state:
    st.session_state.enhanced_assistant = EnhancedVegetableForecastAssistant()
if 'model_retrainer' not in st.session_state:
    st.session_state.model_retrainer = ModelRetrainer()

# Load model and data
if 'model' not in st.session_state:
    model, feat_cols, buffer = load_assets()
    if model is not None:
        st.session_state.model = model
        st.session_state.FEAT_COLS = feat_cols
        st.session_state.BUFFER = buffer

st.title("üá¨üáß Enhanced UK Vegetable Price Forecaster")
st.markdown("**ML-powered forecasting with Semantic RAG, Baseline Comparisons & Model Retraining**")

# Feature availability indicators
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Semantic RAG", "‚úÖ" if EMBEDDINGS_AVAILABLE and FAISS_AVAILABLE else "‚ùå")
with col2:
    st.metric("FAISS Search", "‚úÖ" if FAISS_AVAILABLE else "‚ùå")
with col3:
    st.metric("Baseline Models", "‚úÖ" if STATSFORECAST_AVAILABLE else "‚ùå")
with col4:
    st.metric("Model Retraining", "‚úÖ" if 'model_retrainer' in st.session_state else "‚ùå")

st.markdown("---")

# Create tabs
tab1, tab2, tab3, tab4 = st.tabs(["üìä Forecasting", "üìà Model Comparison", "üìÅ Data Upload & Retrain", "ü§ñ AI Assistant"])

# TAB 1: Forecasting
with tab1:
    st.subheader("üìä Enhanced Price Forecasting")
    
    if 'model' not in st.session_state:
        st.error("Failed to load model or data. Please check file paths.")
        st.stop()

    try:
        available_commodities = sorted(st.session_state.BUFFER.index.get_level_values(0).unique())
        if len(available_commodities) == 0:
            st.error("No commodities found in the data")
            st.stop()
    except Exception as e:
        st.error(f"Error loading commodities: {str(e)}")
        st.stop()

    col1, col2, col3 = st.columns(3)
    with col1:
        selected_commodity = st.selectbox(
            "Choose a vegetable/commodity", 
            available_commodities,
            help="Select the commodity you want to forecast"
        )
    with col2:
        forecast_horizon = st.slider(
            "Forecast horizon (weeks)", 
            min_value=1, 
            max_value=12, 
            value=4,
            help="Number of weeks to forecast ahead"
        )
    with col3:
        show_confidence = st.checkbox(
            "Show confidence intervals", 
            value=True,
            help="Display prediction uncertainty bands"
        )

    if st.button("üìä Generate Enhanced Forecast", type="primary", use_container_width=True):
        with st.spinner("üîÑ Generating enhanced predictions with confidence intervals..."):
            enhanced_result = st.session_state.enhanced_assistant.price_forecast_tool(
                selected_commodity, forecast_horizon
            )
        
        if 'error' not in enhanced_result:
            st.success(f"‚úÖ Enhanced forecast generated for {selected_commodity.title()}")
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown("**Predicted Prices with Confidence:**")
                for i, (pred, (lower, upper)) in enumerate(zip(
                    enhanced_result['predictions'], 
                    enhanced_result['confidence_intervals']
                )):
                    confidence_width = ((upper - lower) / pred) * 100
                    st.metric(
                        f"Week +{i+1}", 
                        f"¬£{pred:.3f}",
                        delta=f"¬±{confidence_width:.1f}%" if show_confidence else None
                    )
            
            with col2:
                st.markdown("**Model Confidence:**")
                confidence_level = enhanced_result.get('model_confidence', 'medium')
                confidence_colors = {'high': 'green', 'medium': 'orange', 'low': 'red'}
                st.markdown(f"<span style='color: {confidence_colors[confidence_level]}'>{confidence_level.upper()}</span>", 
                           unsafe_allow_html=True)
                
                st.markdown(f"**Trend:** {enhanced_result['trend'].title()}")
                st.markdown(f"**Volatility:** {enhanced_result['volatility']}%")
        else:
            st.error(f"Enhanced forecast failed: {enhanced_result['error']}")

# TAB 2: Model Comparison
with tab2:
    st.subheader("üìà Multi-Model Comparison")
    
    if not STATSFORECAST_AVAILABLE:
        st.warning("StatsForecast not available. Install with: `pip install statsforecast`")
        st.markdown("**Available baseline models:** None")
    else:
        st.markdown("**Available models:** LightGBM, AutoARIMA, AutoETS")
    
    col1, col2 = st.columns(2)
    with col1:
        comparison_commodity = st.selectbox(
            "Commodity for comparison", 
            available_commodities if 'available_commodities' in locals() else [],
            key="comparison_commodity"
        )
    with col2:
        comparison_horizon = st.slider(
            "Comparison horizon (weeks)", 
            min_value=1, max_value=8, value=4,
            key="comparison_horizon"
        )
    
    if st.button("üîÑ Generate Model Comparison", type="primary"):
        if STATSFORECAST_AVAILABLE:
            with st.spinner("Generating forecasts from multiple models..."):
                comparison_result = st.session_state.enhanced_assistant.generate_comparison_forecast(
                    comparison_commodity, comparison_horizon
                )
            
            if 'error' not in comparison_result and comparison_result.get('comparison_data') is not None:
                st.success("‚úÖ Multi-model comparison completed")
                
                comparison_df = comparison_result['comparison_data']
                st.markdown("**Model Predictions Comparison:**")
                st.dataframe(comparison_df.round(3))
                
                chart_data = []
                for _, row in comparison_df.iterrows():
                    for model in ['LightGBM', 'AutoARIMA', 'AutoETS']:
                        chart_data.append({
                            'date': row['date'],
                            'price': row[model],
                            'model': model
                        })
                
                chart_df = pd.DataFrame(chart_data)
                
                comparison_chart = alt.Chart(chart_df).mark_line(point=True, strokeWidth=3).encode(
                    x=alt.X('date:T', title='Week Ending'),
                    y=alt.Y('price:Q', title='Price (¬£/kg)'),
                    color=alt.Color('model:N', 
                                  scale=alt.Scale(domain=['LightGBM', 'AutoARIMA', 'AutoETS'], 
                                                range=[COLOR_FORE, COLOR_ARIMA, COLOR_ETS])),
                    tooltip=['date:T', 'price:Q', 'model:N']
                ).properties(
                    width=700,
                    height=400,
                    title=f"{comparison_commodity.title()} - Model Comparison"
                )
                
                st.altair_chart(comparison_chart, use_container_width=True)
                
                st.markdown("**Model Analysis:**")
                lightgbm_preds = comparison_df['LightGBM'].values
                arima_preds = comparison_df['AutoARIMA'].values
                ets_preds = comparison_df['AutoETS'].values
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("LightGBM Avg", f"¬£{np.mean(lightgbm_preds):.3f}")
                    st.metric("LightGBM Vol", f"{np.std(lightgbm_preds)/np.mean(lightgbm_preds)*100:.1f}%")
                with col2:
                    st.metric("AutoARIMA Avg", f"¬£{np.mean(arima_preds):.3f}")
                    st.metric("AutoARIMA Vol", f"{np.std(arima_preds)/np.mean(arima_preds)*100:.1f}%")
                with col3:
                    st.metric("AutoETS Avg", f"¬£{np.mean(ets_preds):.3f}")
                    st.metric("AutoETS Vol", f"{np.std(ets_preds)/np.mean(ets_preds)*100:.1f}%")
                
            else:
                st.error("Model comparison failed or baseline models unavailable")
        else:
            st.error("StatsForecast not available for baseline model comparison")

# TAB 3: Data Upload & Model Retraining
with tab3:
    st.subheader("üìÅ Data Upload & Model Retraining")
    
    st.markdown("""
    **Upload new data to retrain the forecasting model:**
    - Required columns: `commodity`, `week_ending`, `price_gbp_kg`
    - Optional: weather, economic data (will use defaults if missing)
    - Minimum 50 data points required for retraining
    """)
    
    uploaded_file = st.file_uploader(
        "Choose a CSV file", 
        type=['csv', 'xlsx'],
        help="Upload CSV or Excel file with vegetable price data"
    )
    
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith('.csv'):
                new_data = pd.read_csv(uploaded_file)
            else:
                new_data = pd.read_excel(uploaded_file)
            
            st.markdown("**Data Preview:**")
            st.dataframe(new_data.head())
            
            is_valid, validation_message = st.session_state.model_retrainer.validate_uploaded_data(new_data)
            
            if is_valid:
                st.success(f"‚úÖ {validation_message}")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Total Records", len(new_data))
                    st.metric("Commodities", new_data['commodity'].nunique())
                with col2:
                    st.metric("Date Range", f"{new_data['week_ending'].min()} to {new_data['week_ending'].max()}")
                    st.metric("Price Range", f"¬£{new_data['price_gbp_kg'].min():.2f} - ¬£{new_data['price_gbp_kg'].max():.2f}")
                
                if st.button("üîÑ Retrain Model", type="primary"):
                    with st.spinner("Retraining model with new data... This may take a few minutes."):
                        new_model, retrain_message = st.session_state.model_retrainer.retrain_model(new_data)
                    
                    if new_model is not None:
                        st.success(f"‚úÖ {retrain_message}")
                        
                        if st.button("üìä Use Retrained Model"):
                            st.session_state.model = new_model
                            st.success("Model updated! New predictions will use the retrained model.")
                            st.rerun()
                    else:
                        st.error(f"‚ùå {retrain_message}")
            else:
                st.error(f"‚ùå {validation_message}")
                
        except Exception as e:
            st.error(f"Error processing uploaded file: {str(e)}")
    
    st.markdown("---")
    st.markdown("**Current Model Performance:**")
    
    if 'model' in st.session_state:
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Model Type", "LightGBM")
        with col2:
            st.metric("Training Features", len(st.session_state.FEAT_COLS) if 'FEAT_COLS' in st.session_state else "Unknown")
        with col3:
            st.metric("Last Updated", "Original Model")

# TAB 4: AI Assistant
with tab4:
    st.subheader("ü§ñ Enhanced AI Assistant with Semantic RAG")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        rag_status = "‚úÖ Active" if EMBEDDINGS_AVAILABLE else "‚ùå Disabled"
        st.markdown(f"**Semantic RAG:** {rag_status}")
    with col2:
        search_type = "FAISS Vector Search" if FAISS_AVAILABLE else "Keyword Search"
        st.markdown(f"**Search:** {search_type}")
    with col3:
        model_status = "Claude 3.5 Sonnet"
        st.markdown(f"**AI Model:** {model_status}")
    
    chat_container = st.container(height=400)
    
    with chat_container:
        for message in st.session_state.chat_history[-10:]:
            with st.chat_message(message["role"]):
                st.write(message["content"])
    
    if prompt := st.chat_input("Ask about vegetable markets, forecasts, or trading strategies..."):
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.write(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("Analyzing with enhanced semantic search..."):
                response = st.session_state.enhanced_assistant.chat_with_enhanced_claude(
                    prompt, 
                    st.session_state.chat_history
                )
            st.write(response)
        
        st.session_state.chat_history.append({"role": "assistant", "content": response})
    
    st.markdown("**Enhanced Quick Actions:**")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üìä Model Comparison Analysis", use_container_width=True):
            comparison_prompt = f"Compare the performance of LightGBM vs baseline models (AutoARIMA, AutoETS) for {selected_commodity if 'selected_commodity' in locals() else 'potato'} forecasting. Which should I trust more?"
            st.session_state.chat_history.append({"role": "user", "content": comparison_prompt})
            with st.spinner("Generating comparison analysis..."):
                response = st.session_state.enhanced_assistant.chat_with_enhanced_claude(
                    comparison_prompt, st.session_state.chat_history
                )
            st.session_state.chat_history.append({"role": "assistant", "content": response})
            st.rerun()
    
    with col2:
        if st.button("üéØ Trading Strategy", use_container_width=True):
            strategy_prompt = "Based on current market conditions and seasonal patterns, what's the optimal trading strategy for UK vegetables in the next 4 weeks?"
            st.session_state.chat_history.append({"role": "user", "content": strategy_prompt})
            with st.spinner("Developing trading strategy..."):
                response = st.session_state.enhanced_assistant.chat_with_enhanced_claude(
                    strategy_prompt, st.session_state.chat_history
                )
            st.session_state.chat_history.append({"role": "assistant", "content": response})
            st.rerun()
    
    with col3:
        if st.button("‚ö†Ô∏è Risk Assessment", use_container_width=True):
            risk_prompt = "What are the main risks and uncertainties I should consider when using these price forecasts for procurement decisions?"
            st.session_state.chat_history.append({"role": "user", "content": risk_prompt})
            with st.spinner("Assessing risks..."):
                response = st.session_state.enhanced_assistant.chat_with_enhanced_claude(
                    risk_prompt, st.session_state.chat_history
                )
            st.session_state.chat_history.append({"role": "assistant", "content": response})
            st.rerun()

st.markdown("---")
st.write("Enhanced UK Vegetable Price Forecaster - v12")
